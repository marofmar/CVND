# -*- coding: utf-8 -*-
"""lstm_hidden_states_torch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q3AU7GROwHLrPxDK-3Q_gTQ7GBfSvcGN
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import matplotlib.pyplot as plt 
# %matplotlib inline 
torch.manual_seed(2)

# define simple LSTM
from torch.autograd import Variable 
'''
Autograd is a PyTorch package for the differentiation for all operations on Tensors. 
It performs the backpropagation starting from a variable
'''
input_dim =4
hidden_dim = 3 
lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim) 
# this LSTM expects to see 4 values as input and generates 3 values as output 

# make 5 input sequeneces each containing 4 values 
input_list = [torch.randn(1, input_dim) for _ in range(5)] 

# initialize the hidden state 
# (1 layer, 1 batch_size, 3 outputs) 
# first tensor is the hidden state, h0 
# second tensor initializes the cell memory, c0 
h0 = torch.randn(1, 1, hidden_dim) 
c0 = torch.randn(1,1, hidden_dim) 

h0 = Variable(h0) 
c0 = Variable(c0) 

# step through the sequence one element at a time. 
for i in input_list:
    i = Variable(i) 
    out, hidden = lstm(i.view(1,1,-1), (h0, c0))
    print('out: ', out)
    print('hidden: ', hidden)

