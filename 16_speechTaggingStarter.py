"""
CVND 16/30 (Mon 18 Nov 2019) #30DaysofUdacity #CVND #PracticeMakesPerfect
I worked on LSTM for Part of Speech Tagging. 
"""










# -*- coding: utf-8 -*-
"""LSTM_decoder_SpeechTaggingPreprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P6Tn8MG-ZoFwy5mjEEM6dypxA6QaU35D

## LSTM Decoder
In the project, we pass all ourinuts as a sequence to an LSTM. A sequence looks like this: first a feature vector that is extracted from an input image, then a start word, then the enxt work, and  so on.

### Embedding Dimension
The LSTM is defined such that, as it sequentially looks at inputs, it expects that each individual input in a sequence is of a consistent size and so we embed the feature vector and each word so that they are 'embed_size'

## Sequential Inputs
So, an LSTM looks at inputs sequentially. In PyTorch, there are two ways to do this. 

Thie first is pretty intuitive: for all the inputs in a sequence, which in this case would be a feature from an image, a start word, the next word, the next word, and so on (until th end of a sequence/bathc), you loop through each input like so:
"""

# for i in input:
#     out, hidden = lstm(i.view(1,1,-1), hidden)

"""The second approach, which this project uses, is to give the LSTM our entire sequence and have it produce a set of outputs and the last hidden state: 

The first value returned by LSTM is all of the hidden states throughout the sequence. The second is just the most recent hidden state. 

Add the extra 2nd dimension
"""

# inputs = torch.cat(inputs).view(len(inputs), 1, -1) 
# hidden = (torch, randn(1,1,3), torch.randn(1,1,3)) # clean out hidden state
# out, hidden = lstm(inputs, hidden)



"""# LSTM for Part-of-Speech Tagging

Part of speech tagging is the process of determining the category of a word from the words in its surrounding context. You can think of part of speech tagging as a way to go from words to their Mad Libs categories. Mad Libs are incomplete short stories that have many words replaced by blanks. Each blank has a specified word-category, such as `"noun"`, `"verb"`, `"adjective" `, and so on. One player asks another to fill in these blanks (prompted only by the word-category) until they have created a complete, silly story of their own. Here is an exmaple of such categories: 

```text
Today, you'll be learning how to [verb]. It may be a [adjective] process, but I think it will be rewarding!
If you want to take a break you should [verb] and treat yourself to some [plural noun].
```
...and a set of possible words that fall into those categories: 
```text
Today, you'll be learning how to code. It amy be a challenging process, but I think it will be rewarding!
I you want to take a break you should stretch and treat yourself to some puppies.
```

### Why Tag Speech?
Tagging parts of speech is often used to help disambiguate natural language phrases becuase it can be done quickly and with high accuracy. It can help answer: what subject is someone talking about? Tagging can be used for many NLP tasks like creating new sentences using a sequence of tags that make sense toger, filling in a Mad Libs style game, and determining correct pronounciation during speech synthesis. It is also used in information retrieval, and for word disambiguation.

### Preparing the Data

Now, we know that neural networks do not do well with words as input and so our first step will be to prepare our training data and map each word to a numerical value. 

We start by creating a small set of training data, you can see that this is a few simple sentences broken down into a list of words and their corresponding word-tags. Note that the sentences are turned into lowercase words using `lower()` and then split into separate words using spli(), which splits the sentence by whitespace characters. 

### Words to indices 
Then, from this training data, we create a dictionary that maps each unique words in our vocabulary to a numerical value; a unique index `idx`. We do the same for each word-tag, for example: a noun will be represented by the number 1.
"""

# Commented out IPython magic to ensure Python compatibility.
# import resources
import torch 
import torch.nn as nn 
import torch.nn.functional as F 
import torch.optim as optim 
import matplotlib.pyplot as plt 

# %matplotlib inline

# training sentences and their corresponding word-tags
training_data = [
                 ("The cat ate the cheese".lower().split(), ["DET", "NN", "V", "DET", "NN"]),
                 ("She read that book".lower().split(), ["NN", "V", "DET", "NN"]),
                 ("The dog loves art".lower().split(), ["DET", "NN", "V", "NN"]),
                 ("The elephant answers the phone".lower().split(), ["DET", "NN", "V", "DET", "NN"])
                                                        
                 ]

# create a dictionary that maps words to indices 
word2idx = {} 
for sent, tags in training_data:
    for word in sent:
        if word not in word2idx:
            word2idx[word] = len(word2idx)

# create a dictionary that maps tags to indices 
tag2idx = {"DET": 0, "NN": 1, "V": 2}

"""Next, print out the created dictionary to see the words and their numerical values. 

You should see every word in our training set and its index vlaue. Note that the word "the" only appears once because our vocabulary only includes unique words.
"""

print(word2idx)

import numpy as np 
# a helper function for converting a sequence of words to a Tensor of numerical values
# This will be used later in training. 

def prepare_sequence(seq, to_idx):
    idxs = [to_idx[w] for w in seq]
    idxs = np.array(idxs) 
    return torch.from_numpy(idxs)

# check out what prepare_sequence does for one of our training sentences
example_input = prepare_sequence("The dog answers the phone".lower().split(), word2idx)
print(example_input)
